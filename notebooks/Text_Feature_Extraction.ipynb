{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample pipeline for text feature extraction and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.\n",
    "\n",
    "You can adjust the number of categories by giving their names to the dataset loader or setting them to None to get the 20 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [a,b] concatenation is deprecated; use [a;b] instead\n",
      " in depwarn at deprecated.jl:73\n",
      " in oldstyle_vcat_warning at /Applications/Julia-0.4.3.app/Contents/Resources/julia/lib/julia/sys.dylib\n",
      " in vect at abstractarray.jl:38\n",
      " in find_in_path at /Users/cedric/.julia/v0.4/Autoreload/src/files.jl:11\n",
      " in find_file at /Users/cedric/.julia/v0.4/Autoreload/src/files.jl:40\n",
      " in arequire at /Users/cedric/.julia/v0.4/Autoreload/src/Autoreload.jl:50\n",
      " in include at /Applications/Julia-0.4.3.app/Contents/Resources/julia/lib/julia/sys.dylib\n",
      " in include_from_node1 at /Applications/Julia-0.4.3.app/Contents/Resources/julia/lib/julia/sys.dylib\n",
      " in include_string at loading.jl:266\n",
      " in execute_request_0x535c5df2 at /Users/cedric/.julia/v0.4/IJulia/src/execute_request.jl:177\n",
      " in eventloop at /Users/cedric/.julia/v0.4/IJulia/src/IJulia.jl:141\n",
      " in anonymous at task.jl:447\n",
      "while loading /Users/cedric/Programa/Sklearn/notebooks/preamble.jl, in expression starting on line 7\n",
      "WARNING: `require` is deprecated, use `using` or `import` instead\n",
      " in depwarn at deprecated.jl:73\n",
      " [inlined code] from deprecated.jl:694\n",
      " in require at no file:0\n",
      " in arequire at /Users/cedric/.julia/v0.4/Autoreload/src/Autoreload.jl:87\n",
      " in include at /Applications/Julia-0.4.3.app/Contents/Resources/julia/lib/julia/sys.dylib\n",
      " in include_from_node1 at /Applications/Julia-0.4.3.app/Contents/Resources/julia/lib/julia/sys.dylib\n",
      " in include_string at loading.jl:266\n",
      " in execute_request_0x535c5df2 at /Users/cedric/.julia/v0.4/IJulia/src/execute_request.jl:177\n",
      " in eventloop at /Users/cedric/.julia/v0.4/IJulia/src/IJulia.jl:141\n",
      " in anonymous at task.jl:447\n",
      "while loading /Users/cedric/Programa/Sklearn/notebooks/preamble.jl, in expression starting on line 7\n"
     ]
    }
   ],
   "source": [
    "include(\"preamble.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:ASCIIString"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sklearn.datasets.twenty_newsgroups\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"alt.atheism\",\"talk.religion.misc\"]857 documents\n",
      "2 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Python Version Authors: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#                         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#                         Mathieu Blondel <mathieu@mblondel.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "@pyimport2 sklearn.datasets: fetch_20newsgroups\n",
    "@pyimport2 sklearn.feature_extraction.text: CountVectorizer\n",
    "@pyimport2 sklearn.feature_extraction.text: TfidfTransformer\n",
    "@pyimport2 sklearn.linear_model: SGDClassifier\n",
    "@pyimport2 sklearn.grid_search: GridSearchCV\n",
    "@pyimport2 sklearn.pipeline: Pipeline\n",
    "\n",
    "###############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset=\"train\", categories=categories)\n",
    "filenames = data[\"filenames\"]\n",
    "target_names = data[\"target_names\"]\n",
    "println(\"$(length(filenames)) documents\")\n",
    "println(\"$(length(target_names)) categories\")\n",
    "println()\n",
    "\n",
    "cv = CountVectorizer()\n",
    "fit_transform!(cv, data[\"data\"][1:5], [0,0,1])\n",
    "\n",
    "###############################################################################\n",
    "# define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Skcore.Pipeline([\n",
    "    (\"vect\", CountVectorizer()),\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    (\"clf\", SGDClassifier()),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = Dict(\n",
    "    \"vect__max_df\"=> (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    \"vect__ngram_range\"=> ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    \"clf__alpha\"=> (0.00001, 0.000001),\n",
    "    \"clf__penalty\"=> (\"l2\", \"elasticnet\"),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    ")\n",
    "\n",
    "# find the best parameters for both the feature extraction and the\n",
    "# classifier\n",
    "grid_search = Skcore.GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n",
    "\n",
    "println(\"Performing grid search...\")\n",
    "println(\"pipeline:\", [name for (name, _) in pipeline.steps])\n",
    "println(\"parameters:\")\n",
    "println(parameters)\n",
    "t0 = time()\n",
    "N_samples = 100   # number of posts to train on\n",
    "fit!(grid_search, data[\"data\"][1:N_samples], data[\"target\"][1:N_samples])\n",
    "@printf(\"done in %0.3fs\", time() - t0)\n",
    "println()\n",
    "\n",
    "@printf(\"Best score: %0.3f\\n\", grid_search.best_score_)\n",
    "@printf(\"Best parameters set:\\n\")\n",
    "best_parameters = get_params(grid_search.best_estimator_)\n",
    "for param_name in keys(parameters)\n",
    "    println(\"\\t$param_name: $(best_parameters[param_name])\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
