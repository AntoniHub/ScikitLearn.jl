Example | Description
---|---
 <a href="examples/Clustering_Comparison.ipynb"><img src="docs/example_images/Clustering_Comparison.png" alt="# Comparing different clustering algorithms on toy datasets" width="170"> </a>  | This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.
 <a href="examples/Density_Estimation.ipynb"><img src="docs/example_images/Density_Estimation.png" alt="# Density Estimation for a mixture of Gaussians" width="170"> </a>  | This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.
 <a href="examples/Feature_Stacker.ipynb"><img src="docs/example_images/Text_image.png" alt="# Concatenating multiple feature extraction methods" width="170"> </a>  | This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.
 <a href="examples/Outlier_Detection.ipynb"><img src="docs/example_images/Outlier_Detection.png" alt="# Outlier detection with several methods" width="170"> </a>  | This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.
 <a href="examples/Pipeline_PCA_Logistic.ipynb"><img src="docs/example_images/Pipeline_PCA_Logistic.png" alt="# Pipelining: chaining a PCA and a logistic regression" width="170"> </a>  | This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.
 <a href="examples/RBM.ipynb"><img src="docs/example_images/RBM.png" alt="# Restricted Boltzmann Machine features for digit classification" width="170"> </a>  | This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.
 <a href="examples/Simple_1D_Kernel_Density.ipynb"><img src="docs/example_images/Simple_1D_Kernel_Density.png" alt="# Simple 1D Kernel Density Estimation" width="170"> </a>  | This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.
 <a href="examples/Text_Feature_Extraction.ipynb"><img src="docs/example_images/Text_image.png" alt="# Sample pipeline for text feature extraction and evaluation" width="170"> </a>  | This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.
 <a href="examples/Two_Class_Adaboost.ipynb"><img src="docs/example_images/Two_Class_Adaboost.png" alt="# Two Class Adaboost" width="170"> </a>  | This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.
 <a href="examples/Underfitting_vs_Overfitting.ipynb"><img src="docs/example_images/Underfitting_vs_Overfitting.png" alt="# Underfitting vs. Overfitting" width="170"> </a>  | This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.
