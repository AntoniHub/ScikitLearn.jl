The examples below are in Julia, there are many more [in Python](http://scikit-learn.org/stable/auto_examples/index.html)

Example | Description
---|---
 <a href="https://github.com/cstjean/ScikitLearn.jl/tree/master/examples/Clustering_Comparison.ipynb"><img src="https://github.com/cstjean/ScikitLearn.jl/tree/master/docs/example_images/Clustering_Comparison.png" alt="# Comparing different clustering algorithms on toy datasets" width="170"> </a>  | This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.
 <a href="https://github.com/cstjean/ScikitLearn.jl/tree/master/examples/Density_Estimation.ipynb"><img src="https://github.com/cstjean/ScikitLearn.jl/tree/master/docs/example_images/Density_Estimation.png" alt="# Density Estimation for a mixture of Gaussians" width="170"> </a>  | Plot the density estimation of a mixture of two Gaussians. Data is generated from two Gaussians with different centers and covariance matrices.
 <a href="https://github.com/cstjean/ScikitLearn.jl/tree/master/examples/Feature_Stacker.ipynb"><img src="https://github.com/cstjean/ScikitLearn.jl/tree/master/docs/example_images/Text_image.png" alt="# Concatenating multiple feature extraction methods" width="170"> </a>  | In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use FeatureUnion to combine features obtained by PCA and univariate selection.  Combining features using this transformer has the benefit that it allows cross validation and grid searches over the whole process.  The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.
 <a href="https://github.com/cstjean/ScikitLearn.jl/tree/master/examples/Outlier_Detection.ipynb"><img src="https://github.com/cstjean/ScikitLearn.jl/tree/master/docs/example_images/Outlier_Detection.png" alt="# Outlier detection with several methods" width="170"> </a>  | When the amount of contamination is known, this example illustrates two different ways of performing Novelty and Outlier Detection:  * based on a robust estimator of covariance, which is assuming that the data are Gaussian distributed and performs better than the One-Class SVM in that case. * using the One-Class SVM and its ability to capture the shape of the data set, hence performing better when the data is strongly non-Gaussian, i.e. with two well-separated clusters;  The ground truth about inliers and outliers is given by the points colors while the orange-filled area indicates which points are reported as inliers by each method.  Here, we assume that we know the fraction of outliers in the datasets. Thus rather than using the predict method of the objects, we set the threshold on the decision_function to separate out the corresponding fraction.
 <a href="https://github.com/cstjean/ScikitLearn.jl/tree/master/examples/Pipeline_PCA_Logistic.ipynb"><img src="https://github.com/cstjean/ScikitLearn.jl/tree/master/docs/example_images/Pipeline_PCA_Logistic.png" alt="# Pipelining: chaining a PCA and a logistic regression" width="170"> </a>  | The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.  We use a GridSearchCV to set the dimensionality of the PCA
 <a href="https://github.com/cstjean/ScikitLearn.jl/tree/master/examples/RBM.ipynb"><img src="https://github.com/cstjean/ScikitLearn.jl/tree/master/docs/example_images/RBM.png" alt="# Restricted Boltzmann Machine features for digit classification" width="170"> </a>  | For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model (BernoulliRBM) can perform effective non-linear feature extraction.  In order to learn good latent representations from a small dataset, we artificially generate more labeled data by perturbing the training data with linear shifts of 1 pixel in each direction.  This example shows how to build a classification pipeline with a BernoulliRBM feature extractor and a LogisticRegression classifier. The hyperparameters of the entire model (learning rate, hidden layer size, regularization) were optimized by grid search, but the search is not reproduced here because of runtime constraints.  Logistic regression on raw pixel values is presented for comparison. The example shows that the features extracted by the BernoulliRBM help improve the classification accuracy.
 <a href="https://github.com/cstjean/ScikitLearn.jl/tree/master/examples/Simple_1D_Kernel_Density.ipynb"><img src="https://github.com/cstjean/ScikitLearn.jl/tree/master/docs/example_images/Simple_1D_Kernel_Density.png" alt="# Simple 1D Kernel Density Estimation" width="170"> </a>  | The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.  We use a GridSearchCV to set the dimensionality of the PCA
 <a href="https://github.com/cstjean/ScikitLearn.jl/tree/master/examples/Text_Feature_Extraction.ipynb"><img src="https://github.com/cstjean/ScikitLearn.jl/tree/master/docs/example_images/Text_image.png" alt="# Sample pipeline for text feature extraction and evaluation" width="170"> </a>  | The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.  You can adjust the number of categories by giving their names to the dataset loader or setting them to None to get the 20 of them.
 <a href="https://github.com/cstjean/ScikitLearn.jl/tree/master/examples/Two_Class_Adaboost.ipynb"><img src="https://github.com/cstjean/ScikitLearn.jl/tree/master/docs/example_images/Two_Class_Adaboost.png" alt="# Two Class Adaboost" width="170"> </a>  | This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two Gaussian quantiles clusters (see `sklearn.datasets.make_gaussian_quantiles`) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value.
 <a href="https://github.com/cstjean/ScikitLearn.jl/tree/master/examples/Underfitting_vs_Overfitting.ipynb"><img src="https://github.com/cstjean/ScikitLearn.jl/tree/master/docs/example_images/Underfitting_vs_Overfitting.png" alt="# Underfitting vs. Overfitting" width="170"> </a>  | This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will overfit the training data, i.e. it learns the noise of the training data. We evaluate quantitatively overfitting / underfitting by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.
