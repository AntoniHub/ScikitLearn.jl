{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Class Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two “Gaussian quantiles” clusters (see `sklearn.datasets.make_gaussian_quantiles`) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.tree.tree.DecisionTreeClassifier'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Where should Ndgrid be?\n",
    "#include(\"../src/Ndgrid.jl\")\n",
    "\n",
    "using PyPlot\n",
    "using Sklearn\n",
    "using Sklearn.Utils: meshgrid\n",
    "\n",
    "@sk_import datasets: make_gaussian_quantiles\n",
    "@sk_import ensemble: AdaBoostClassifier\n",
    "@sk_import tree: DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1, y1 = make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=200, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=300, n_features=2,\n",
    "                                 n_classes=2, random_state=1);\n",
    "X = vcat(X1, X2);\n",
    "y = vcat(y1, -y2 + 1);\n",
    "\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                                algorithm=\"SAMME\",\n",
    "                                n_estimators=200)\n",
    "fit!(bdt, X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundaries\n",
    "figure(figsize=(10, 5))\n",
    "\n",
    "subplot(121)\n",
    "\n",
    "plot_colors = \"br\"\n",
    "plot_step = 0.02\n",
    "class_names = \"AB\"\n",
    "\n",
    "x_min, x_max = minimum(X[:, 1]) - 1, maximum(X[:, 1]) + 1\n",
    "y_min, y_max = minimum(X[:, 2]) - 1, maximum(X[:, 2]) + 1\n",
    "xx, yy = meshgrid(x_min:plot_step:x_max, y_min:plot_step:y_max)\n",
    "\n",
    "Z = predict(bdt, hcat(xx[:], yy[:]));\n",
    "\n",
    "Z = reshape(Z, size(xx)...);\n",
    "\n",
    "cs = contourf(xx, yy, Z, cmap=get_cmap(\"Paired\"))\n",
    "plt[:axis](\"tight\")\n",
    "\n",
    "# Plot the training set\n",
    "for (i, n, c) in zip(0:1, class_names, plot_colors)\n",
    "    idx = find(y .== i)\n",
    "    scatter(X[idx, 1], X[idx, 2],\n",
    "            c=string(c), cmap=get_cmap(\"Paired\"),\n",
    "            label=\"Class $n\")\n",
    "end\n",
    "\n",
    "xlim(x_min, x_max)\n",
    "ylim(y_min, y_max)\n",
    "legend(loc=\"upper right\")\n",
    "xlabel(\"x\")\n",
    "ylabel(\"y\")\n",
    "title(\"Decision Boundary\")\n",
    "\n",
    "subplot(122)\n",
    "# Plot the two-class decision scores\n",
    "twoclass_output = decision_function(bdt, X)\n",
    "plot_range = (minimum(twoclass_output), maximum(twoclass_output))\n",
    "subplot(122)\n",
    "for (i, n, c) in zip(0:1, class_names, plot_colors)\n",
    "    plt[:hist](twoclass_output[y .== i],\n",
    "               bins=10,\n",
    "               range=plot_range,\n",
    "               facecolor=string(c),\n",
    "               label=\"Class $n\",\n",
    "               alpha=.5)\n",
    "end\n",
    "\n",
    "x1, x2, y1, y2 = axis()\n",
    "axis((x1, x2, y1, y2 * 1.2))\n",
    "legend(loc=\"upper right\")\n",
    "ylabel(\"Samples\")\n",
    "xlabel(\"Score\")\n",
    "title(\"Decision Scores\")\n",
    "\n",
    "tight_layout()\n",
    "subplots_adjust(wspace=0.35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
